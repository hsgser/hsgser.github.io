<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction | Dang Nguyen </title> <meta name="author" content="Dang Nguyen"> <meta name="description" content="Dang Nguyen's homepage "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%87%A9%E2%80%8B&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hsgser.github.io/blog/2023/SADSW/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Dang</span> Nguyen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction</h1> <p class="post-meta"> Created in September 12, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/optimal-transport"> <i class="fa-solid fa-hashtag fa-sm"></i> optimal-transport</a>   <a href="/blog/tag/sliced-wasserstein"> <i class="fa-solid fa-hashtag fa-sm"></i> sliced-wasserstein</a>   <a href="/blog/tag/point-cloud"> <i class="fa-solid fa-hashtag fa-sm"></i> point-cloud</a>   <a href="/blog/tag/self-attention"> <i class="fa-solid fa-hashtag fa-sm"></i> self-attention</a>   ·   <a href="/blog/category/conference"> <i class="fa-solid fa-tag fa-sm"></i> conference</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>Table of contents</strong></p> <ul> <li><a href="#introduction">Introduction</a></li> <li> <a href="#background">Background</a> <ul> <li><a href="#optimal-transport">Optimal Transport</a></li> <li><a href="#max-sliced-wasserstein-distance">Max Sliced Wasserstein Distance</a></li> <li><a href="#amortized-projection-optimization">Amortized Projection Optimization</a></li> </ul> </li> <li> <a href="#self-attention-amortized-distributional-projection-optimization">Self-Attention Amortized Distributional Projection Optimization</a> <ul> <li><a href="#amortized-distributional-projection-optimization">Amortized Distributional Projection Optimization</a></li> <li><a href="#self-attention-amortized-models">Self-Attention Amortized Models</a></li> </ul> </li> <li><a href="#experiments">Experiments</a></li> <li><a href="#conclusion">Conclusion</a></li> <li><a href="#references">References</a></li> </ul> <h2 id="introduction">Introduction</h2> <p>Based on the closed-form solution of Wasserstein distance in one dimension, Sliced Wasserstein (SW) has been utilized successfully in point-cloud representation learning [1, 2] due to its computational efficiency. However, the downside of SW is that it treats all projections the same due to the usage of a uniform distribution over projecting directions. Thus, max sliced Wasserstein (Max-SW) [3] distance was proposed as a solution for less discriminative projections of sliced Wasserstein (SW) distance. In applications that have various independent pairs of probability measures, amortized projection optimization [4] was introduced to predict the “max” projecting directions given two input measures instead of using projected gradient ascent multiple times. Despite being efficient, Max-SW and its amortized version cannot guarantee metricity property due to the sub-optimality of the projected gradient ascent and the amortization gap. Therefore, in this paper, we propose to replace Max-SW with distributional sliced Wasserstein distance with von Mises-Fisher (vMF) projecting distribution (v-DSW). Since v-DSW is a metric with any non-degenerate vMF distribution, its amortized version can guarantee the metricity when performing amortization. Furthermore, current amortized models are not permutation invariant and symmetric, thus they are not suitable to deal with set-based data (e.g. point-clouds). To address the issue, we design amortized models based on self-attention architecture. In particular, we adopt efficient self-attention architectures to make the computation linear in the number of supports. With the two improvements, we derive self-attention amortized distributional projection optimization and show its appealing performance in point-cloud reconstruction and its downstream applications.</p> <h2 id="background">Background</h2> <h3 id="optimal-transport">Optimal Transport</h3> <p>We denote a point-cloud of $m$ points \(x_1,\ldots,x_m \in \mathbb{R}^d\) (\(d \geq 1\)) as \(X=(x_1,\ldots,x_m) \in \mathbb{R}^{dm}\) which is a vector of a concatenation of all points in the point-cloud. We denote the set of all possible point-clouds as \(\mathcal{X} \subset \mathbb{R}^{dm}\). In the point-cloud representation learning, we want to estimate \(f_\phi:\mathcal{X} \to \mathcal{Z}\) (\(\phi \in \Phi\)) jointly with a function \(g_\gamma:\mathcal{Z} \to \mathcal{X}\) (\(\gamma \in \Gamma\)) given a point-cloud dataset \(p(X)\) (distribution over set of poin-clouds \(\mathcal{X}\)) by minimizing the objective:</p> \[\begin{equation} \min_{\phi \in \Phi,\gamma \in \Gamma}\mathbb{E}_{X \sim p(X)} \mathcal{D}(X,g_\gamma (f_\phi(X))). \end{equation}\] <p>Here, \(\mathcal{D}\) is a metric between two point-clouds.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pc_reconstruction.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pc_reconstruction.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pc_reconstruction.PNG-1400.webp"></source> <img src="/assets/img/pc_reconstruction.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 1. Overview of Point-cloud Reconstruction.</p> <h3 id="max-sliced-wasserstein-distance">Max Sliced Wasserstein Distance</h3> <p>Max sliced Wasserstein (Max-SW) distance between \(\mu \in \mathcal{P}_p(\mathbb{R}^d)\) and \(\nu\in \mathcal{P}_p(\mathbb{R}^d)\) is:</p> \[\begin{equation} \text{Max-SW}_p(\mu,\nu) = \max_{\theta \in \mathbb{S}^{d - 1}} W_p(\theta\sharp \mu,\theta \sharp \nu), \end{equation}\] <p>where the Wasserstein distance has a closed form on one dimension which is</p> \[\begin{equation} W_p(\mu,\nu) = \left( \int_0^1 |F_\mu^{-1}(z) - F_{\nu}^{-1}(z)|^{p} dz \right)^{1/p}, \end{equation}\] <p>with \(F^{-1}_{\mu}\) and \(F^{-1}_{\nu}\) are the inverse CDF of \(\mu\) and \(\nu\) respectively.</p> <p><strong>Max sliced point-cloud reconstruction:</strong> Instead of solving all optimization problems independently, an amortized model is trained to predict optimal solutions to all problems. Given a parametric function \(a_\psi: \mathcal{X}\times \mathcal{X} \to \mathbb{S}^{d-1}\) (\(\psi \in \Psi\)), the amortized objective is:</p> \[\begin{equation} \min_{\phi \in \Phi,\gamma \in \Gamma }\mathbb{E} \left[\max_{\theta \in \mathbb{S}^{d-1}}W_p(\theta \sharp P_X,\theta \sharp P_{g_\gamma (f_\phi(X))})\right], \end{equation}\] <p>the one-dimensional Wasserstein between two projected point-clouds can be solved with the time complexity \(\mathcal{O}(m\log m)\).</p> <h3 id="amortized-projection-optimization">Amortized Projection Optimization</h3> <p>Instead of solving all optimization problems independently, an amortized model is trained to predict optimal solutions to all problems. Given a parametric function \(a_\psi: \mathcal{X}\times \mathcal{X} \to \mathbb{S}^{d-1}\) (\(\psi \in \Psi\)), the amortized objective is:</p> \[\begin{equation} \min_{\phi \in \Phi,\gamma \in \Gamma}\max_{ \psi \in \Psi}\mathbb{E}_{X \sim p(X)}[W_p(\theta_{\psi,\gamma,\phi}\sharp P_X,\theta_{\psi,\gamma,\phi} \sharp P_{g_\gamma (f_\phi(X))})], \end{equation}\] <p>where \(\theta_{\psi,\gamma,\phi} = a_\psi(X,g_\gamma (f_\phi(X)))\).</p> <h2 id="self-attention-amortized-distributional-projection-optimization">Self-Attention Amortized Distributional Projection Optimization</h2> <h3 id="amortized-distributional-projection-optimization">Amortized Distributional Projection Optimization</h3> <p>Amortized optimization often leads to sub-optimality. Hence, it loses the metricity property since the Max-SW only obtains the identity of indiscernibles at the global optimum. Therefore, we propose to predict an entire distribution over projecting directions.</p> \[\begin{equation} \min_{\phi \in \Phi,\gamma \in \Gamma}\max_{ \psi \in \Psi}\mathbb{E}_{X \sim p(X)} \Big(\mathbb{E}_{\theta \sim \text{vMF}(\epsilon_{\psi,\gamma,\phi},\kappa)} W_p^p(\theta \sharp P_X,\theta \sharp P_{g_\gamma (f_\phi(X))})\Big)^{\frac{1}{p}}, \end{equation}\] <p>where \(\epsilon_{\psi,\gamma,\phi} = a_\psi(X,g_\gamma (f_\phi(X)))\), \(\text{vMF}(\epsilon,\kappa)\) is the von Mises Fisher distribution with the mean location parameter \(\epsilon \in \mathbb{S}^{d-1}\) and the concentration parameter \(\kappa &gt; 0\), and</p> \[\begin{equation} \text{v-DSW}_p(\mu,\nu;\kappa) =\max_{\epsilon \in \mathbb{S}^{d-1}} \Big(\mathbb{E}_{\theta \sim \text{vMF}(\epsilon,\kappa)} \text{W}_p^p(\theta \sharp \mu,\theta \sharp \nu) \Big)^{\frac{1}{p}} \end{equation}\] <p>is the von Mises-Fisher distributional sliced Wasserstein distance.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/amsw_avsw.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/amsw_avsw.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/amsw_avsw.PNG-1400.webp"></source> <img src="/assets/img/amsw_avsw.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 2. The difference between amortized projection optimization and amortized distributional projection optimization.</p> <h3 id="self-attention-amortized-models">Self-Attention Amortized Models</h3> <p>Based on the self-attention mechanism, we introduce the self-attention amortized model which is permutation invariant and symmetric. Given \(X,Y \in \mathbb{R}^{dm}\), the <em>self-attention amortized model</em> is defined as:</p> \[\begin{equation} a_\psi (X,Y)=\frac{\mathcal{A}_{\zeta}(X'^\top)^\top \boldsymbol{1}_{m} + \mathcal{A}_{\zeta}(Y'^\top)^\top \boldsymbol{1}_{m}}{||\mathcal{A}_{\zeta}(X'^\top)^\top \boldsymbol{1}_{m} + \mathcal{A}_{\zeta}(Y'^\top)^\top \boldsymbol{1}_{m}||_2}, \end{equation}\] <p>where \(X'\) and \(Y'\) are matrices of size \(d\times m\) that are reshaped from the concatenated vectors \(X\) and \(Y\) of size \(dm\), \(\boldsymbol{1}_{m}\) is the $m$-dimensional vector whose all entries are \(1\), and \(\mathcal{A}_{\zeta}(\cdot)\) is linear (efficient) attention module [5, 6] for preserving near-linear complexity.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/amortized_models.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/amortized_models.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/amortized_models.PNG-1400.webp"></source> <img src="/assets/img/amortized_models.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 3. Visualization of an amortized model that is not symmetric and permutation invariant in two dimensions.</p> <h2 id="experiments">Experiments</h2> <p>To verify the effectiveness of our proposal, we evaluate our methods on the point-cloud reconstruction task and its two downstream tasks including transfer learning and point-cloud generation (please see our papers for more details).</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/reconstruction_quantitative.PNG" alt="reconstruction_quantitative" style="display:block; margin-left:auto; margin-right:auto"> Table 1. Reconstruction and transfer learning performance on the ModelNet40 dataset. CD and SW are multiplied by 100.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/reconstruction_qualitative-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/reconstruction_qualitative-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/reconstruction_qualitative-1400.webp"></source> <img src="/assets/img/reconstruction_qualitative.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 4. Qualitative results of reconstructing point-clouds in the ShapeNet Core-55 dataset. From top to bottom, the point-clouds are input, SW, Max-SW, v-DSW, and $\mathcal{L}\mathcal{A}$v-DSW respectively.</p> <h2 id="conclusion">Conclusion</h2> <p>In this paper, we have proposed a self-attention amortized distributional projection optimization framework which uses a self-attention amortized model to predict the best discriminative distribution over projecting direction for each pair of probability measures. The efficient self-attention mechanism helps to inject the geometric inductive biases which are permutation invariance and symmetry into the amortized model while remaining fast computation. Furthermore, the amortized distribution projection optimization framework guarantees the metricity for all pairs of probability measures while the amortization gap still exists. On the experimental side, we compare the new proposed framework to the conventional amortized projection optimization framework and other widely-used distances in the point-cloud reconstruction application and its two downstream tasks including transfer learning and point-cloud generation to show the superior performance of the proposed framework. For further information, please refer to our work at <a href="https://proceedings.mlr.press/v202/nguyen23e/nguyen23e.pdf" rel="external nofollow noopener" target="_blank">https://proceedings.mlr.press/v202/nguyen23e/nguyen23e.pdf</a>.</p> <h2 id="references">References</h2> <p>[1] Nguyen, T., Pham, Q.-H., Le, T., Pham, T., Ho, N., and Hua,B.-S. Point-set distances for learning representations of 3d point clouds. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.</p> <p>[2] Naderializadeh, N., Comer, J., Andrews, R., Hoffmann, H., and Kolouri, S. Pooling by sliced-Wasserstein embedding. Advances in Neural Information Processing Systems, 34, 2021.</p> <p>[3] Deshpande, I., Hu, Y.-T., Sun, R., Pyrros, A., Siddiqui, N., Koyejo, S., Zhao, Z., Forsyth, D., and Schwing, A. G. Max-sliced Wasserstein distance and its use for GANs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 10648–10656, 2019.</p> <p>[4] Nguyen, K. and Ho, N. Amortized projection optimization for sliced Wasserstein generative models. Advances in Neural Information Processing Systems, 2022.</p> <p>[5] Shen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 3531–3539, 2021.</p> <p>[6] Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/mPOT/">Improving Mini-batch Optimal Transport via Partial Transportation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/BoMbOT/">On Transportation of Mini-batches: A Hierarchical Approach</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Dang Nguyen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>